import torch

class PGD:
    def __init__(self, model, norm, device='cpu', epsilon=0.05, alpha=0.001, num_iter=100, targeted=True):
        """
        model      : the network to attack (expects logits out; will apply your normalization inside forward)
        norm       : a callable that normalizes inputs for your model (e.g. transforms that do (x-mean)/std)
        epsilon    : maximum perturbation (in the same scale as your inputs, e.g. [0,1])
        alpha      : per‐step step‐size
        num_iter   : number of PGD steps
        targeted   : if True, does a *targeted* attack (minimize loss); otherwise untargeted (maximize loss)
        """
        self.device = device
        self.model = model
        self.norm = norm
        self.epsilon = epsilon
        self.alpha = alpha
        self.num_iter = num_iter
        self.targeted = targeted
        self.loss_fn = torch.nn.CrossEntropyLoss()

    def attack(self, images, labels):
        images = images.to(self.device)
        labels = labels.to(self.device)

        best_loss = float('inf')
        best_adv_images = images.clone()

        # Starting point: a random point in the epsilon-ball uniform in [−ε, +ε]
        delta = torch.empty_like(images).uniform_(-self.epsilon, self.epsilon)
        adv_images = torch.clamp(images + delta, 0, 1).detach()

        adv_images.requires_grad_(True)

        # PGD loop
        for _ in range(self.num_iter):
            # zero any existing gradients
            if adv_images.grad is not None:
                adv_images.grad.zero_()

            # forward pass
            outputs = self.model(self.norm(adv_images)) # model expects normalized inputs
            loss = self.loss_fn(outputs, labels)

            # for targeted attack, descend loss; for untargeted, ascend
            loss = loss if self.targeted else -loss
            loss.backward()

            # take a step in the sign‐gradient direction
            grad_sign = adv_images.grad.data.sign()
            adv_images = adv_images + self.alpha * grad_sign

            # project back into the ε‐ball around the original images
            adv_images = torch.min(
                torch.max(adv_images, images - self.epsilon),
                images + self.epsilon
            )
            # and clamp to [0,1]
            adv_images = torch.clamp(adv_images, 0, 1).detach()
            adv_images.requires_grad_(True)

            # Check if the current adversarial image is better than the best one
            if loss.item() < best_loss:
                best_loss = loss.item()
                best_adv_images = adv_images.clone()

        return best_adv_images
    
class PGDWhiteBox(PGD):
    '''
    A modified version of the PGD attack that attacks the detection model as well as
    the target classifier. This is done by including the Anomaly Score generated by the
    detection model in the loss function.
    '''
    def __init__(self, model, norm, device, detector, epsilon=0.05, alpha=0.001, num_iter=100, targeted=True, k=0.1):
        super().__init__(model, norm, device, epsilon, alpha, num_iter, targeted)
        self.detector = detector
        self.k = k # weight for the anomaly score in the loss function
        
    def attack(self, images, labels):
        images = images.to(self.device)
        labels = labels.to(self.device)

        best_loss = float('inf')
        best_adv_images = images.clone()

        # Starting point: a random point in the epsilon-ball uniform in [−ε, +ε]
        delta = torch.empty_like(images).uniform_(-self.epsilon, self.epsilon)
        adv_images = torch.clamp(images + delta, 0, 1).detach()

        adv_images.requires_grad_(True)

        # PGD loop
        for _ in range(self.num_iter):
            # zero any existing gradients
            if adv_images.grad is not None:
                adv_images.grad.zero_()

            # forward pass
            outputs = self.model(self.norm(adv_images)) # model expects normalized inputs
            re, pu = self.detector.forward(self.norm(adv_images).to(self.device))
            AS = (re - self.detector.e_mean) / self.detector.e_std + (pu - self.detector.v_mean) / self.detector.v_std

            loss = self.loss_fn(outputs, labels) 
            # for targeted attack, descend loss; for untargeted, ascend
            loss = loss if self.targeted else -loss
            loss = 0*loss - self.k * AS
            loss.backward()

            # take a step in the sign‐gradient direction
            grad_sign = adv_images.grad.data.sign()
            adv_images = adv_images + self.alpha * grad_sign

            # project back into the ε‐ball around the original images
            adv_images = torch.min(
                torch.max(adv_images, images - self.epsilon),
                images + self.epsilon
            )
            # and clamp to [0,1]
            adv_images = torch.clamp(adv_images, 0, 1).detach()
            adv_images.requires_grad_(True)

            # Check if the current adversarial image is better than the best one
            if loss.item() < best_loss:
                best_loss = loss.item()
                best_adv_images = adv_images.clone()

        return best_adv_images