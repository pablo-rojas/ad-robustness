import torch
import torch.nn.functional as F
from src.eval_utils import sd_statistic
class PGD:
    def __init__(self, model, norm, device='cpu', epsilon=0.05, step_size=0.001, num_iter=100, targeted=True, random_start=False):
        """
        model      : the network to attack (expects logits out; will apply your normalization inside forward)
        norm       : a callable that normalizes inputs for your model (e.g. transforms that do (x-mean)/std)
        epsilon    : maximum perturbation (in the same scale as your inputs, e.g. [0,1])
        step_size      : per‐step step‐size
        num_iter   : number of PGD steps
        targeted   : if True, does a *targeted* attack (minimize loss); otherwise untargeted (maximize loss)
        random_start : if True, starts from a random point in the epsilon-ball around the original image
        """
        self.device = device
        self.model = model
        self.norm = norm
        self.epsilon = epsilon
        self.step_size = step_size
        self.num_iter = num_iter
        self.targeted = targeted
        self.random_start = random_start
        self.loss_fn = torch.nn.CrossEntropyLoss()

    def attack(self, images, labels):
        images = images.to(self.device)
        labels = labels.to(self.device)

        best_loss = -float('inf')
        best_adv_images = images.clone()

        # If random_start is True, start from a random point in the epsilon-ball
        if self.random_start:
            # Starting point: a random point in the epsilon-ball uniform in [−ε, +ε]
            delta = torch.empty_like(images).uniform_(-self.epsilon, self.epsilon)
            adv_images = torch.clamp(images + delta, 0, 1).detach()
        else:
            # Starting point: the original image
            adv_images = images.clone().detach()

        adv_images.requires_grad_(True)

        # PGD loop
        for _ in range(self.num_iter):
            # zero any existing gradients
            if adv_images.grad is not None:
                adv_images.grad.zero_()

            # forward pass
            outputs = self.model(self.norm(adv_images)) # model expects normalized inputs
            loss = self.loss_fn(outputs, labels)

            # for targeted attack, descend loss; for untargeted, ascend
            loss = -loss if self.targeted else loss
            self.model.zero_grad()
            loss.backward()

            # take a step in the sign‐gradient direction
            grad_sign = adv_images.grad.data.sign()
            adv_images = adv_images + self.step_size * grad_sign

            # project back into the ε‐ball around the original images
            adv_images = torch.min(
                torch.max(adv_images, images - self.epsilon),
                images + self.epsilon
            )
            # and clamp to [0,1]
            adv_images = torch.clamp(adv_images, 0, 1).detach()
            adv_images.requires_grad_(True)

            # Check if the current adversarial image is better than the best one
            if loss.item() > best_loss:
                best_loss = loss.item()
                best_adv_images = adv_images.clone()

        return best_adv_images
    
class PGDus(PGD):
    '''
    A modified version of the PGD attack that attacks the detection model as well as
    the target classifier. This is done by including the Anomaly Score generated by the
    detection model in the loss function.
    '''
    def __init__(self, model, norm, device, detector, epsilon=0.05, step_size=0.001, num_iter=100, targeted=True, random_start=False, k=0.1, norm_grad=True):
        super().__init__(model, norm, device, epsilon, step_size, num_iter, targeted, random_start)
        self.detector = detector
        self.k = k # weight for the anomaly score in the loss function
        self.norm_grad = norm_grad # whether to normalize the gradients or not
        
    def attack(self, images, labels):
        images = images.to(self.device)
        labels = labels.to(self.device)

        best_loss = -float('inf')
        best_adv_images = images.clone().detach()

        # If random_start is True, start from a random point in the epsilon-ball
        if self.random_start:
            # Starting point: a random point in the epsilon-ball uniform in [−ε, +ε]
            delta = torch.empty_like(images).uniform_(-self.epsilon, self.epsilon)
            adv_images = torch.clamp(images + delta, 0, 1).detach()
        else:
            # Starting point: the original image
            adv_images = images.clone().detach()
        adv_images.requires_grad_(True)

        # PGD loop
        for _ in range(self.num_iter):
            # zero any existing gradients
            if adv_images.grad is not None:
                adv_images.grad.zero_()
            self.model.zero_grad()
            self.detector.zero_grad()

            # forward pass
            outputs = self.model(self.norm(adv_images)) # model expects normalized inputs
            re, pu = self.detector.forward(self.norm(adv_images))
            AS = (re - self.detector.e_mean) / self.detector.e_std + (pu - self.detector.v_mean) / self.detector.v_std

            loss = self.loss_fn(outputs, labels) 
            # for targeted attack, descend loss; for untargeted, ascend
            loss = -loss if self.targeted else loss

            if self.norm_grad:
                g_cls = torch.autograd.grad(loss, adv_images)[0]
                g_det = torch.autograd.grad(AS, adv_images)[0]
                gradient = g_cls/(g_cls.norm() + 1e-16) - self.k+g_det/(g_det.norm()  + 1e-16)
                grad_sign = gradient.sign()
                loss = loss - self.k * AS
            else:
                loss = loss - self.k * AS
                loss.backward()
                grad_sign = adv_images.grad.data.sign()

            adv_images = adv_images + self.step_size * grad_sign

            # project back into the ε‐ball around the original images
            adv_images = torch.min(
                torch.max(adv_images, images - self.epsilon),
                images + self.epsilon
            )
            # and clamp to [0,1]
            adv_images = torch.clamp(adv_images, 0, 1).detach()
            adv_images.requires_grad_(True)

            # Check if the current adversarial image is better than the best one
            if loss.item() > best_loss:
                best_loss = loss.item()
                best_adv_images = adv_images.clone().detach()

        return best_adv_images
    

class PGDacgan(PGD):
    '''
    A modified version of the PGD attack that attacks the detection model as well as
    the target classifier. This is done by including the Anomaly Score generated by the
    detection model in the loss function.
    '''
    def __init__(self, model, norm, device, detector, epsilon=0.05, step_size=0.001, num_iter=100, targeted=True, random_start=False, k=0.1, norm_grad=True):
        super().__init__(model, norm, device, epsilon, step_size, num_iter, targeted, random_start)
        self.detector = detector
        self.k = k # weight for the anomaly score in the loss function
        self.norm_grad = norm_grad # whether to normalize the gradients or not
        
    def attack(self, images, labels):
        images = images.to(self.device)
        labels = labels.to(self.device)

        best_loss = -float('inf')
        best_adv_images = images.clone()

        # If random_start is True, start from a random point in the epsilon-ball
        if self.random_start:
            # Starting point: a random point in the epsilon-ball uniform in [−ε, +ε]
            delta = torch.empty_like(images).uniform_(-self.epsilon, self.epsilon)
            adv_images = torch.clamp(images + delta, 0, 1).detach()
        else:
            # Starting point: the original image
            adv_images = images.clone().detach()
        adv_images.requires_grad_(True)

        # PGD loop
        for _ in range(self.num_iter):
            # zero any existing gradients
            if adv_images.grad is not None:
                adv_images.grad.zero_()
            self.model.zero_grad()
            self.detector.discriminator.zero_grad()

            # forward pass
            outputs = self.model(self.norm(adv_images)) # model expects normalized inputs
            AS = sd_statistic(self.detector.discriminator(adv_images), outputs.argmax(1))                     # now log‐probs            
            loss = self.loss_fn(outputs, labels) 

            # for targeted attack, descend loss; for untargeted, ascend
            loss = -loss if self.targeted else loss

            if self.norm_grad:
                g_cls = torch.autograd.grad(loss, adv_images)[0]
                g_det = torch.autograd.grad(AS, adv_images)[0]
                gradient = g_cls/(g_cls.norm() + 1e-16) - self.k+g_det/(g_det.norm()  + 1e-16)
                grad_sign = gradient.sign()
                loss = loss - self.k * AS
            else:
                loss = loss - self.k * AS
                loss.backward()
                grad_sign = adv_images.grad.data.sign()

            adv_images = adv_images + self.step_size * grad_sign

            # project back into the ε‐ball around the original images
            adv_images = torch.min(
                torch.max(adv_images, images - self.epsilon),
                images + self.epsilon
            )
            # and clamp to [0,1]
            adv_images = torch.clamp(adv_images, 0, 1).detach()
            adv_images.requires_grad_(True)

            # Check if the current adversarial image is better than the best one
            if loss.item() > best_loss:
                best_loss = loss.item()
                best_adv_images = adv_images.clone()

        return best_adv_images
    
